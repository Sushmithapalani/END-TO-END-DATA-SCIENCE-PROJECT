
import pandas as pd
import numpy as np
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
import os # Import the os module

print("Starting Data Preprocessing and Model Training...")

# 1. Load Data
print("Loading California Housing dataset...")
housing = fetch_california_housing(as_frame=True)
df = housing.frame
df['MEDV'] = housing.target # Add the target variable to the DataFrame
print("Dataset loaded successfully.")
print(f"Dataset shape: {df.shape}")
print("First 5 rows of the dataset:")
print(df.head())

# 2. Exploratory Data Analysis (EDA) - Basic checks
print("\nPerforming basic EDA...")
print("\nDataset Info:")
df.info()
print("\nMissing values (sum per column):")
print(df.isnull().sum()) # Check for missing values (California Housing is clean)
print("\nDescriptive statistics:")
print(df.describe())

# Visualize distributions (example for a few features)
plt.figure(figsize=(12, 8))
df.hist(bins=50, figsize=(20, 15))
plt.suptitle("Feature Distributions")
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

# Correlation matrix
plt.figure(figsize=(10, 8))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Correlation Matrix")
plt.show()

# 3. Feature Engineering (Minimal for this dataset, but good to include the step)
# No specific feature engineering needed for this clean dataset right now.
# However, if you had features like 'address', you might engineer 'latitude', 'longitude', 'zip_code', etc.

# 4. Data Splitting
X = df.drop('MEDV', axis=1) # Features
y = df['MEDV'] # Target variable (Median House Value)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print(f"\nTraining set shape: {X_train.shape}, Testing set shape: {X_test.shape}")

# 5. Feature Scaling
# It's good practice to scale numerical features for many ML models.
print("Applying StandardScaler to features...")
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
print("Features scaled.")

# 6. Model Training
print("Training Linear Regression model...")
model = LinearRegression()
model.fit(X_train_scaled, y_train)
print("Model training complete.")

# 7. Model Evaluation
print("\nEvaluating model performance...")
y_pred_train = model.predict(X_train_scaled)
y_pred_test = model.predict(X_test_scaled)

mse_train = mean_squared_error(y_train, y_pred_train)
rmse_train = np.sqrt(mse_train)
r2_train = r2_score(y_train, y_pred_train)

mse_test = mean_squared_error(y_test, y_pred_test)
rmse_test = np.sqrt(mse_test)
r2_test = r2_score(y_test, y_pred_test)

print(f"Training RMSE: {rmse_train:.2f}")
print(f"Training R-squared: {r2_train:.2f}")
print(f"Testing RMSE: {rmse_test:.2f}")
print(f"Testing R-squared: {r2_test:.2f}")

# 8. Model Persistence
# Create the 'model' directory if it doesn't exist
os.makedirs('model', exist_ok=True)

# Save the trained model and the scaler object
print("\nSaving model and scaler...")
joblib.dump(model, 'model/linear_regression_model.pkl')
joblib.dump(scaler, 'model/scaler.pkl')
print("Model and scaler saved successfully to 'model/' directory.")

print("\nData Preprocessing and Model Training complete.")
